{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PGC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importando as bibliotecas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "regex = r\"[-'a-zA-ZÀ-ÖØ-öø-ÿ0-9]+\"   # raw string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abrindo o corpus e armazenando o conteúdo completo em *content* e as linhas em *paragraphs*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de palavras: 83919\n"
     ]
    }
   ],
   "source": [
    "fileName = \"corpus-textos-DC.txt\"\n",
    "\n",
    "document = open(fileName,'r', encoding='utf-8')\n",
    "content  = document.read()\n",
    "\n",
    "document = open(fileName,'r', encoding='utf-8')\n",
    "paragraphs = document.readlines()\n",
    "\n",
    "words = re.findall(regex, content)\n",
    "\n",
    "print (f\"Quantidade de palavras: {len(words)}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removendo parágrafos em branco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "for paragraph in paragraphs:\n",
    "    if paragraph == \"\\n\":\n",
    "        paragraphs.remove(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de parágrafos: 1682\n"
     ]
    }
   ],
   "source": [
    "print(f\"Quantidade de parágrafos: {len(paragraphs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantidade e frequência de palavras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do vocabulário: 12425\n"
     ]
    }
   ],
   "source": [
    "frequencies = dict([])\n",
    "for w in words:\n",
    "    w = w.lower()\n",
    "    if w not in frequencies:\n",
    "        frequencies[w] = 0\n",
    "    frequencies[w] += 1\n",
    "print (f\"Tamanho do vocabulário: {len(frequencies)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4091 - de\n",
      "2570 - a\n",
      "2554 - e\n",
      "2179 - que\n",
      "2140 - o\n",
      "1274 - em\n",
      "1194 - da\n",
      "1167 - do\n",
      "1070 - é\n",
      "1013 - um\n",
      "916 - para\n",
      "806 - com\n",
      "799 - uma\n",
      "681 - os\n",
      "660 - por\n",
      "644 - como\n",
      "622 - no\n",
      "615 - na\n",
      "586 - não\n",
      "512 - se\n"
     ]
    }
   ],
   "source": [
    "fs = sorted(frequencies, key=frequencies.get, reverse=True)\n",
    "for i in range(0,20):\n",
    "    print (f\"{frequencies[fs[i]]} - {fs[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantidade e frequência de palavras após a remoção de stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do vocabulário (sem stopwords): 12255\n"
     ]
    }
   ],
   "source": [
    "stopwordsPTfile = open(\"stopwords-pt.txt\",'r', encoding='utf-8')\n",
    "stopwords       = set([]) \n",
    "for s in stopwordsPTfile.readlines():\n",
    "    stopwords.add(s.strip().lower())\n",
    "    \n",
    "words       = re.findall(regex, content)\n",
    "frequencies = dict([])\n",
    "\n",
    "# quantidade de vezes no documento\n",
    "for w in words:\n",
    "    w = w.lower()\n",
    "    if w not in stopwords:\n",
    "        if w not in frequencies:\n",
    "            frequencies[w] = 0\n",
    "        frequencies[w] += 1\n",
    "        \n",
    "print (f\"Tamanho do vocabulário (sem stopwords): {len(frequencies)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210 - pode\n",
      "205 - ciência\n",
      "201 - sobre\n",
      "188 - água\n",
      "174 - acessibilidade\n",
      "141 - anos\n",
      "137 - forma\n",
      "133 - além\n",
      "102 - parte\n",
      "101 - podem\n",
      "99 - pessoas\n",
      "99 - exemplo\n",
      "97 - vida\n",
      "96 - luz\n",
      "95 - dia\n",
      "93 - bem\n",
      "92 - história\n",
      "91 - química\n",
      "91 - cada\n",
      "90 - tempo\n"
     ]
    }
   ],
   "source": [
    "fs = sorted(frequencies, key=frequencies.get, reverse=True)\n",
    "for i in range(0,20):\n",
    "    print (f\"{frequencies[fs[i]]} - {fs[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "textos = []\n",
    "palavras = []\n",
    "\n",
    "for word in words:\n",
    "    if(word != \"----------------------------------------------------------------------------------\"):\n",
    "        palavras.append(word)\n",
    "    else:\n",
    "        textos.append(palavras)\n",
    "        palavras = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'que', 'é', 'um', 'podcast', 'É', 'tipo', 'rádio', 'Como', 'faço', 'para', 'começar', 'a', 'ouvir', 'acessibilidade', 'cachorro', 'da', 'raça', 'Golden', 'Retriever', 'em', 'um', 'parque', 'com', 'a', 'língua', 'para', 'fora', 'e', 'usando', 'um', 'headphone', 'Certamente', 'ouvindo', 'o', 'Laços', 'podcast', 'O', 'termo', 'podcast', 'vem', 'da', 'junção', 'de', 'pod', 'personal', 'on', 'demand', 'e', 'cast', 'de', 'broadcast', 'sendo', 'portanto', 'uma', 'transmissão', 'pessoal', 'sob', 'demanda', 'É', 'como', 'um', 'programa', 'de', 'rádio', 'mas', 'você', 'pode', 'ouvir', 'pela', 'internet', 'ou', 'baixar', 'o', 'programa', 'quando', 'quiser', 'não', 'precisa', 'sintonizar', 'uma', 'frequência', 'de', 'rádio', 'em', 'um', 'horário', 'específico', 'para', 'ouvir', 'Podcasts', 'são', 'uma', 'ótima', 'forma', 'de', 'se', 'manter', 'atualizado', 'sobre', 'diversos', 'assuntos', 'de', 'seu', 'interesse', 'enquanto', 'realiza', 'outras', 'tarefas', 'como', 'limpar', 'a', 'casa', 'comer', 'caminhar', 'ou', 'pilotar', 'máquinas', 'agrícolas', 'Por', 'ser', 'apenas', 'áudio', 'não', 'demanda', 'a', 'mesma', 'atenção', 'necessária', 'para', 'se', 'assistir', 'a', 'um', 'vídeo', 'por', 'exemplo', 'No', 'transporte', 'público', 'enquanto', 'dirige', 'enquanto', 'espera', 'por', 'um', 'compromisso', 'qualquer', 'hora', 'é', 'hora', 'Não', 'há', 'uma', 'ordem', 'certa', 'para', 'começar', 'a', 'ouvir', 'nem', 'há', 'a', 'necessidade', 'de', 'ouvir', 'o', 'anterior', 'para', 'se', 'ouvir', 'o', 'próximo', 'basta', 'escolher', 'um', 'episódio', 'e', 'começar', 'Podcasts', 'geralmente', 'tratam', 'de', 'um', 'tema', 'central', 'com', 'cada', 'episódio', 'falando', 'de', 'um', 'assunto', 'específico', 'relacionado', 'a', 'esse', 'tema', 'Conteúdo', 'nerd', 'Nerdcast', 'política', 'internacional', 'Xadrez', 'Verbal', 'cinema', 'RapaduraCast', 'games', 'Jogabilidade', 'direito', 'Salvo', 'Melhor', 'Juízo', 'design', 'Visual', 'Mente', 'mulheres', 'que', 'marcaram', 'a', 'história', 'Ponto', 'G', 'e', 'é', 'claro', 'ciência', 'Falaremos', 'mais', 'sobre', 'podcasts', 'de', 'ciência', 'ao', 'longo', 'do', 'texto', 'Gostou', 'e', 'quer', 'começar', 'a', 'ouvir', 'mas', 'ainda', 'não', 'sabe', 'como', 'Siga', 'os', 'passos', '1', 'Baixe', 'e', 'instale', 'um', 'agregador', 'de', 'podcasts', 'no', 'seu', 'celular', 'como', 'o', 'iTunes', 'WeCast', 'Google', 'Podcasts', 'ou', 'o', 'Podcast', 'Addict', 'Usaremos', 'o', 'Podcast', 'Addict', 'a', 'partir', 'de', 'agora', '2', 'Abra', 'o', 'aplicativo', 'e', 'clique', 'no', 'sinal', 'de', 'no', 'canto', 'superior', 'direito', 'Nesta', 'tela', 'é', 'possível', 'ver', 'as', 'tendências', 'novidades', 'mais', 'baixados', 'em', 'cada', 'categoria', 'etc', 'Na', 'barra', 'superior', 'é', 'possível', 'ver', 'o', 'ícone', 'da', 'lupa', 'para', 'pesquisar', 'e', 'o', 'ícone', 'do', 'feed', 'RSS', 'para', 'assinar', 'um', 'podcast', 'pelo', 'link', 'do', 'feed', 'Vamos', 'abrir', 'a', 'lupa', 'e', 'pesquisar', 'um', 'podcast', '3', 'Pesquise', 'pelo', 'Naruhodo', 'podcast', 'comandado', 'por', 'Altay', 'de', 'Souza', 'e', 'Ken', 'Fujioka', 'que', 'se', 'propõe', 'a', 'responder', 'perguntas', 'como', 'Estralar', 'os', 'dedos', 'faz', 'mal', 'Porque', 'algumas', 'pessoas', 'passam', 'mal', 'ao', 'ver', 'sangue', 'e', 'Por', 'que', 'temos', 'pintas', 'em', 'nosso', 'corpo', 'sempre', 'com', 'embasamento', 'científico', 'em', 'episódios', 'com', 'aproximadamente', '30', 'minutos', '4', 'Nos', 'resultados', 'da', 'pesquisa', 'você', 'pode', 'ver', 'que', 'o', 'Naruhodo', 'tem', 'mais', 'de', '130', 'episódios', 'e', 'mais', 'de', 'vinte', 'mil', 'assinantes', 'Assine', 'o', 'podcast', 'se', 'quiser', 'ver', 'atualizações', 'e', 'então', 'abra', 'a', 'lista', 'de', 'episódios', '5', 'Selecione', 'algum', 'episódio', 'que', 'seja', 'do', 'seu', 'interesse', 'É', 'possível', 'ouvir', 'por', 'streaming', 'sem', 'a', 'necessidade', 'de', 'baixar', 'mas', 'para', 'isso', 'conexão', 'com', 'a', 'internet', 'é', 'necessária', 'enquanto', 'estiver', 'ouvindo', 'A', 'outra', 'opção', 'é', 'baixar', 'o', 'episódio', '6', 'Abra', 'as', 'opções', 'no', 'canto', 'superior', 'esquerdo', 'Em', 'Últimos', 'episódios', 'você', 'pode', 'ver', 'os', 'episódios', 'dos', 'últimos', 'sete', 'dias', 'de', 'todos', 'os', 'podcasts', 'em', 'que', 'estiver', 'inscrito', 'Em', 'Episódios', 'baixados', 'você', 'pode', 'ver', 'todos', 'os', 'episódios', 'que', 'foram', 'baixados', 'no', 'celular', 'Selecione', 'esta', 'opção', 'e', 'escolha', 'um', 'para', 'ouvir', '7', 'O', 'player', 'do', 'aplicativo', 'é', 'bem', 'simples', 'você', 'pode', 'pausar', 'continuar', 'adiantar', 'voltar', 'ir', 'para', 'o', 'próximo', 'ou', 'ao', 'anterior', 'caso', 'tenha', 'feito', 'uma', 'playlist', 'ligar', 'um', 'timer', 'e', 'alterar', 'a', 'velocidade', 'de', 'reprodução', 'ideal', 'para', 'quando', 'você', 'estiver', 'com', 'pouco', 'tempo', 'Nas', 'configurações', 'você', 'ainda', 'pode', 'ligar', 'desligar', 'a', 'atualização', 'automática', 'o', 'download', 'automático', 'remoção', 'automática', 'de', 'episódios', 'vistos', 'etc', 'Outras', 'formas', 'de', 'ouvir', 'A', 'extensão', 'do', 'Google', 'Chrome', 'PodStation', 'permite', 'buscar', 'podcasts', 'pelo', 'nome', 'procurar', 'pelo', 'feed', 'assinar', 'ver', 'os', 'últimos', 'episódios', 'dos', 'podcasts', 'que', 'tiver', 'assinado', 'executar', 'sem', 'baixar', 'ou', 'baixar', 'e', 'executar', 'pausar', 'alterar', 'a', 'velocidade', 'de', 'reprodução', 'etc', 'acessibilidade', 'Print', 'Screen', 'da', 'extensão', 'PodStation', 'visualizando', 'todos', 'os', 'episódios', 'do', 'SciCast', 'Ideal', 'para', 'ouvir', 'os', 'dois', 'maiores', 'podcasts', 'de', 'ciência', 'do', 'Brasil', 'Scicast', 'criado', 'pelo', 'falecido', 'Silmar', 'Geremia', 'hoje', 'conta', 'com', 'Tarik', 'Fernandes', 'Fernando', 'Malta', 'Marcelo', 'Guaxinim', 'dentre', 'outros', 'membros', 'e', 'convidados', 'Provavelmente', 'o', 'maior', 'podcast', 'de', 'ciência', 'do', 'Brasil', 'aborda', 'diversos', 'temas', 'de', 'forma', 'sempre', 'bem', 'fundamentada', 'e', 'bem-humorada', 'porque', 'a', 'ciência', 'tem', 'que', 'ser', 'divertida', 'Dragões', 'de', 'Garagem', 'criado', 'por', 'Luciano', 'Queiroz', 'é', 'um', 'dos', 'maiores', 'podcasts', 'de', 'ciência', 'do', 'Brasil', 'Além', 'do', 'podcast', 'o', 'Dragões', 'publica', 'uma', 'vez', 'por', 'semana', 'as', 'Cientirinhas', 'tirinhas', 'com', 'temática', 'científica', 'e', 'tem', 'um', 'canal', 'no', 'YouTube', 'onde', 'vídeos', 'sobre', 'notícias', 'são', 'postados', 'toda', 'sexta-feira', 'Ainda', 'é', 'possível', 'ouvir', 'pelo', 'Deezer', 'ou', 'pelo', 'Spotify', 'pesquisando', 'pelo', 'gênero', 'podcasts', 'acessibilidade', 'Print', 'Screen', 'do', 'Spotify', 'mostrando', 'o', 'resultado', 'da', 'busca', 'pelo', 'termo', 'podcast', 'Baixar', 'o', 'arquivo', 'mp3', 'diretamente', 'pelo', 'site', 'acessibilidade', 'Print', 'Screen', 'do', 'site', 'do', 'NeuroCast', 'podcast', 'sobre', 'neurociência', 'da', 'UFABC', 'Ouvir', 'pelo', 'Soundcloud', 'acessibilidade', 'Print', 'Screen', 'do', 'SoundCloud', 'do', 'podcast', 'Bug', 'Bites', 'sobre', 'entomologia', 'Ou', 'até', 'mesmo', 'pelo', 'Youtube', 'acessibilidade', 'Print', 'Screen', 'da', 'playlist', 'Xadrez', 'Verbal', 'Podcast', 'no', 'canal', 'do', 'Xadrez', 'Verbal', 'no', 'YouTube', 'Ainda', 'falando', 'sobre', 'podcasts', 'de', 'ciência', 'importante', 'citar', 'os', 'excelentes', 'PodEntender', 'Alô', 'Ciência', 'e', 'os', 'podcast', 'feitos', 'em', 'Universidades', 'como', 'o', 'Paideia', 'da', 'UFSCAR', 'o', 'Front', 'da', 'Ciência', 'da', 'UFRGS', 'o', 'Oxigênio', 'da', 'Unicamp', 'e', 'o', 'NeuroCast', 'da', 'UFABC', 'Podcasts', 'também', 'podem', 'ser', 'usados', 'na', 'educação', 'professores', 'disponibilizando', 'suas', 'aulas', 'em', 'áudio', 'para', 'que', 'os', 'alunos', 'revisem', 'a', 'matéria', 'em', 'qualquer', 'lugar', 'Pensando', 'em', 'criar', 'seu', 'próprio', 'podcast', 'Não', 'deixe', 'de', 'ver', 'os', 'tutoriais', 'do', 'Mundo', 'Podcast', 'aproveite', 'e', 'ouça', 'o', 'excelente', 'PodProgramar', 'Este', 'trecho', 'foi', 'apenas', 'uma', 'referência', 'ao', 'programa', 'Choque', 'de', 'cultura', 'Evite', 'distrações', 'ao', 'dirigir', 'Marcelo', 'Pena']\n"
     ]
    }
   ],
   "source": [
    "print(textos[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "textos2 = []\n",
    "paragrafos = []\n",
    "\n",
    "for paragraph in paragraphs:\n",
    "    if(paragraph != \"----------------------------------------------------------------------------------\\n\"):\n",
    "        paragrafos.append(paragraph)\n",
    "    else:\n",
    "        textos2.append(paragrafos)\n",
    "        paragrafos = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Livro - Professor, para que estudo isso?\\n', '#acessibilidade Capa do livro “Professor, para que estudo isso?”\\n', 'Muitas vezes, especialmente no Ensino Médio, nos questionamos porque temos que estudar certas matérias. Foi procurando respostas para esta pergunta, especialmente para a área de ciências e matemática, que o grupo WhatSci! escreveu um livro. Veja mais na resenha abaixo, escrito pela jornalista Nicete Campos:\\n', '“O livro “Professor, para que estudo isso?”, da Editora Livraria da Física, contou com a organização dos pesquisadores Cassiano M. Aono, Gabriela Dias da Silva e Paula Homem de Mello da Universidade Federal do ABC, atendendo a uma demanda que se faz presente em sala de aula. Este livro, que foi escrito por alunos de graduação, pós-graduação e professores desta Universidade, foi produzido baseado nas questões selecionadas do Ensino Médio, em função da alegação de que as diferentes e diversas temáticas sobre ciência e tecnologia não são abordadas. Na contramão deste suposto, o livro ensina que os conceitos básicos por traz desses temas são explorados sim, e que tais conceitos estão presentes e interconectam as diferentes disciplinas. Um elo que une as ciências naturais é a linguagem nela encontrada, ou seja, a universal matemática, vista por ângulos tão simples e relacionados com tantas áreas prazerosas, que fica fácil gostar e aplicar, pois convergem para as disciplinas menos temidas pelos estudantes.\\n', 'Alguns subtítulos são aqui exemplificados: “Existe química” entre duas pessoas?”; “Por que não existem digitais nem íris iguais?”; “É possível sair de um buraco negro?”; “As coisas têm cheiro no espaço?”, ou ainda, “É possível a vida baseada em outro elemento químico que não o carbono?”.\\n', 'Esta primeira edição, lançada no fim de 2017, conta com 188 páginas. É um esforço conjunto de professores e alunos de diferentes cursos da Universidade Federal do ABC. A iniciativa de extensão à comunidade escolar vai além, pois os “curiosos” não escolares podem também se beneficiar com esta leitura lúdica, clara e objetiva.”\\n', 'Nicete Campos – Jornalista (MTB – 16.921)\\n', 'Informações de contato estão disponíveis na página da Editora Livraria da Física. Clique aqui para adquirir o livro\\n', 'admin\\n']\n"
     ]
    }
   ],
   "source": [
    "print(textos2[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "postagging = spacy.load('pt_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsedData = postagging(textos2[i][0][:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O Paradoxo de Simpson te mostra que nem tudo é o que parece\n"
     ]
    }
   ],
   "source": [
    "print(parsedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palavra: O \n",
      "Lemma: O \n",
      "Postag: DET \n",
      "\n",
      "Palavra: Paradoxo \n",
      "Lemma: Paradoxo \n",
      "Postag: PROPN \n",
      "\n",
      "Palavra: de \n",
      "Lemma: de \n",
      "Postag: ADP \n",
      "\n",
      "Palavra: Simpson \n",
      "Lemma: Simpson \n",
      "Postag: PROPN \n",
      "\n",
      "Palavra: te \n",
      "Lemma: te \n",
      "Postag: PROPN \n",
      "\n",
      "Palavra: mostra \n",
      "Lemma: mostrar \n",
      "Postag: VERB \n",
      "\n",
      "Palavra: que \n",
      "Lemma: que \n",
      "Postag: SCONJ \n",
      "\n",
      "Palavra: nem \n",
      "Lemma: nem \n",
      "Postag: ADV \n",
      "\n",
      "Palavra: tudo \n",
      "Lemma: tudo \n",
      "Postag: PRON \n",
      "\n",
      "Palavra: é \n",
      "Lemma: ser \n",
      "Postag: VERB \n",
      "\n",
      "Palavra: o \n",
      "Lemma: o \n",
      "Postag: PRON \n",
      "\n",
      "Palavra: que \n",
      "Lemma: que \n",
      "Postag: PRON \n",
      "\n",
      "Palavra: parece \n",
      "Lemma: parecer \n",
      "Postag: VERB \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for j,word in enumerate(parsedData):\n",
    "    print(\"Palavra:\",word.text,\"\\nLemma:\",word.lemma_,\"\\nPostag:\",word.pos_,\"\\n\")#,word.tag_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paradoxo LOC\n",
      "Simpson PER\n"
     ]
    }
   ],
   "source": [
    "for word in parsedData:\n",
    "    if word.ent_type_:\n",
    "        print(word.text, word.ent_type_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#\n",
      "ou\n",
      "seja\n",
      "negro\n",
      "#\n",
      "ou\n",
      "seja\n",
      "O\n",
      "O\n",
      "1\n",
      "#\n",
      "à\n",
      "#\n",
      "humano\n",
      "Ou\n",
      "seja\n",
      "o\n",
      "O\n",
      "#\n",
      "colorida\n",
      "#\n",
      "o\n",
      "o\n",
      "O\n",
      "#\n",
      "o\n",
      "o\n",
      "o\n",
      "ou\n",
      "seja\n",
      "ou\n",
      "seja\n",
      "#\n",
      "O\n",
      "ou\n",
      "seja\n",
      "o\n",
      "#\n",
      "o\n",
      "o\n",
      "#\n",
      "#\n",
      "ou\n",
      "seja\n",
      "o\n",
      "ou\n",
      "seja\n",
      "#\n",
      "o\n",
      "o\n",
      "#\n",
      "#\n",
      "#\n",
      "o\n",
      "#\n",
      "o\n",
      "#\n",
      "Ou\n",
      "seja\n",
      "#\n",
      "e\n",
      "O\n",
      "#\n",
      "o\n",
      "o\n",
      "bom\n",
      "ou\n",
      "seja\n",
      "bom\n",
      "bom\n",
      "#\n",
      "o\n",
      "#\n",
      "#\n",
      "no\n",
      "e\n",
      "O\n",
      "#\n",
      "o\n",
      "#\n",
      "Ou\n",
      "seja\n",
      "#\n",
      "thinking\n",
      "face\n",
      "o\n",
      "e\n",
      "o\n",
      "o\n",
      "#\n",
      "#\n",
      "ou\n",
      "seja\n",
      "ou\n",
      "seja\n",
      "ou\n",
      "seja\n",
      "#\n",
      "#\n",
      "#\n",
      "o\n",
      "#\n",
      "#\n",
      "#\n",
      "o\n",
      "no\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "#\n",
      "ou\n",
      "seja\n",
      "e\n",
      "#\n",
      "o\n",
      "das\n",
      "ou\n",
      "seja\n",
      "#\n",
      "#\n",
      "verde\n",
      "o\n",
      "#\n",
      "e\n",
      "#\n",
      "online\n",
      "este\n",
      "#\n",
      "ou\n",
      "seja\n",
      "O\n",
      "#\n",
      "humano\n",
      "O\n",
      "o\n",
      "pH\n",
      "o\n",
      "#\n",
      "e\n",
      "o\n",
      "#\n",
      "quadrados\n",
      "20/11/2018\n",
      "e\n",
      "civil\n",
      "O\n",
      "#\n",
      "o\n",
      "à\n",
      "o\n",
      "#\n",
      "o\n",
      "#\n",
      "#\n",
      "vistos\n",
      "no\n",
      "o\n",
      "tratando\n",
      "o\n",
      "o\n",
      "e\n",
      "houve\n",
      "e\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "#\n",
      "grande\n",
      "o\n",
      "o\n",
      "#\n",
      "ou\n",
      "seja\n",
      "o\n",
      "O\n",
      "#\n",
      "e\n",
      "e\n",
      "o\n",
      "O\n",
      "ou\n",
      "seja\n",
      "o\n",
      "#\n",
      "O\n",
      "o\n",
      "#\n",
      "O\n",
      "o\n",
      "#\n",
      "#\n",
      "o\n",
      "no\n",
      "O\n",
      "#\n",
      "o\n",
      "o\n",
      "ou\n",
      "seja\n",
      "ou\n",
      "seja\n",
      "no\n",
      "O\n",
      "o\n",
      "#\n",
      "ou\n",
      "seja\n",
      "#\n",
      "o\n",
      "ou\n",
      "seja\n",
      "#\n",
      "#\n",
      "#\n",
      "o\n",
      "#\n",
      "o\n",
      "ou\n",
      "seja\n",
      "superior\n",
      "superior\n",
      "o\n",
      "#\n",
      "o\n",
      "O\n",
      "e\n",
      "cor\n",
      "#\n",
      "humano\n",
      "o\n",
      "1\n",
      "3\n",
      "#\n",
      "o\n",
      "à\n",
      "#\n",
      "o\n",
      "#\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "O\n",
      "o\n",
      "#\n",
      "#\n",
      "carbono\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "o\n",
      "#\n",
      "o\n",
      "o\n",
      "ou\n",
      "seja\n",
      "quo\n",
      "#\n",
      "cercado\n",
      "#\n",
      "#\n",
      "o\n",
      "da\n",
      "à\n",
      "#\n",
      "e\n",
      "no\n",
      "o\n",
      "pública\n",
      "Médio\n",
      "#\n",
      "o\n",
      "O\n",
      "O\n",
      "#\n",
      "0\n",
      "#\n",
      "e\n",
      "o\n",
      "#\n",
      "Fria\n",
      "#\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "#\n",
      "o\n",
      "negro\n",
      "o\n",
      "O\n",
      "#\n",
      "o\n",
      "#\n",
      "#\n",
      "#\n",
      "o\n",
      "ou\n",
      "seja\n",
      "#\n",
      "O\n",
      "#\n",
      "#\n",
      "ou\n",
      "seja\n",
      "O\n",
      "o\n",
      "o\n",
      "O\n",
      "O\n",
      "#\n",
      "#\n",
      "o\n",
      "print\n",
      "o\n",
      "#\n",
      "o\n",
      "o\n",
      "o\n",
      "#\n",
      "#\n",
      "ou\n",
      "seja\n",
      "ex\n",
      "o\n",
      "#\n",
      "Ou\n",
      "seja\n",
      "#\n",
      "e\n",
      "#\n",
      "ou\n",
      "seja\n",
      "O\n",
      "#\n",
      "ou\n",
      "seja\n",
      "ou\n",
      "seja\n",
      "ou\n",
      "seja\n",
      "o\n",
      "#\n",
      "O\n",
      "Ou\n",
      "seja\n",
      "o\n",
      "ou\n",
      "seja\n",
      "o\n",
      "o\n",
      "#\n",
      "#\n",
      "#\n",
      "divertir\n",
      "O\n",
      "#\n",
      "O\n",
      "e\n",
      "no\n",
      "e\n",
      "Superior\n",
      "superior\n",
      "#\n",
      "ou\n",
      "seja\n",
      "e\n",
      "Ou\n",
      "seja\n",
      "o\n",
      "isto\n",
      "é\n",
      "o\n",
      "#\n",
      "o\n",
      "—\n",
      "o\n",
      "ou\n",
      "seja\n",
      "das\n",
      "O\n",
      "#\n",
      "Ou\n",
      "seja\n",
      "ou\n",
      "seja\n",
      "humano\n",
      "humano\n",
      "#\n",
      "O\n",
      "#\n",
      "#\n",
      "#\n",
      "das\n",
      "o\n",
      "o\n",
      "negro\n",
      "negro\n",
      "negro\n",
      "isto\n",
      "é\n",
      "o\n",
      "o\n",
      "na\n",
      "#\n",
      "no\n",
      "O\n",
      "#\n",
      "o\n",
      "#\n",
      "O\n",
      "e\n",
      "que\n",
      "ou\n",
      "seja\n",
      "ou\n",
      "seja\n",
      "o\n",
      "#\n",
      "O\n",
      "Ou\n",
      "seja\n",
      "o\n",
      "à\n",
      "ou\n",
      "seja\n",
      "#\n",
      "do\n",
      "sensíveis\n",
      "#\n",
      "à\n",
      "#\n",
      "ou\n",
      "seja\n",
      "o\n",
      "o\n",
      "#\n",
      "o\n",
      "O\n",
      "#\n",
      "o\n",
      "ou\n",
      "seja\n",
      "#\n",
      "O\n",
      "das\n",
      "40.31 segundos\n"
     ]
    }
   ],
   "source": [
    "t=time.perf_counter()\n",
    "\n",
    "nNounsTotal = []\n",
    "nAdjTotal = []\n",
    "nVerbTotal = []\n",
    "nNesTotal = []\n",
    "xTotal = []\n",
    "\n",
    "for k in range (len(textos)):\n",
    "    nNouns = 0\n",
    "    nAdj = 0\n",
    "    nVerb = 0\n",
    "    nNes = 0\n",
    "    x = 0\n",
    "    for l in range (len(textos2[k])):        \n",
    "        parsedData = postagging(textos2[k][l])\n",
    "        for j,palavra in enumerate(parsedData):\n",
    "            tag = palavra.pos_\n",
    "            if tag in [\"NOUN\",\"PROPN\"]:\n",
    "                nNouns += 1\n",
    "            elif tag in [\"ADJ\"]:\n",
    "                nAdj += 1\n",
    "            elif tag in [\"VERB\"]:\n",
    "                nVerb += 1\n",
    "            elif tag in [\"X\"]:\n",
    "                x += 1\n",
    "                print(palavra)\n",
    "            if palavra.ent_type_:\n",
    "                nNes += 1\n",
    "            #print(\"Palavra:\",palavra.text,\"\\nLemma:\",palavra.lemma_,\"\\nPostag:\",palavra.pos_,\"\\n\") #word.tag_)\n",
    "    nNounsTotal.append(nNouns)\n",
    "    nAdjTotal.append(nAdj)\n",
    "    nVerbTotal.append(nVerb)\n",
    "    nNesTotal.append(nNes)\n",
    "    xTotal.append(x)\n",
    "    \n",
    "print(round(time.perf_counter()-t,3), \"segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[137, 223, 322, 120, 227, 116, 142, 103, 185, 382, 140, 85, 103, 187, 100, 139, 102, 198, 219, 264, 224, 118, 84, 134, 251, 114, 141, 530, 102, 99, 103, 107, 73, 271, 182, 116, 169, 145, 158, 227, 248, 169, 94, 201, 370, 128, 323, 178, 144, 172, 358, 222, 127, 174, 116, 347, 284, 358, 170, 113, 122, 494, 293, 364, 229, 297, 110, 144, 230, 366, 263, 285, 222, 133, 234, 98, 120, 188, 326, 243, 189, 482, 477, 591, 331, 392, 416, 172, 329, 172, 431, 262, 425, 232, 225, 233, 210, 280, 205, 155, 320, 226, 255, 358, 230, 285, 150, 202, 173, 383, 107, 246, 285, 182, 187, 195, 182, 202]\n",
      "[30, 37, 42, 20, 40, 25, 26, 14, 25, 70, 26, 22, 43, 26, 25, 24, 38, 49, 40, 38, 63, 35, 27, 38, 38, 22, 40, 104, 32, 26, 19, 33, 20, 63, 54, 38, 40, 36, 27, 61, 36, 36, 21, 57, 67, 26, 88, 53, 40, 34, 59, 61, 26, 57, 36, 62, 69, 67, 37, 28, 24, 116, 86, 66, 69, 61, 29, 34, 52, 67, 60, 66, 45, 31, 71, 19, 26, 44, 47, 44, 47, 77, 104, 71, 122, 78, 71, 39, 125, 32, 104, 33, 65, 46, 60, 65, 46, 85, 70, 34, 64, 65, 61, 108, 69, 71, 52, 45, 34, 85, 36, 87, 82, 67, 72, 61, 35, 46]\n",
      "[41, 60, 152, 43, 55, 53, 56, 54, 86, 140, 60, 48, 31, 63, 50, 64, 40, 145, 75, 78, 68, 73, 32, 64, 105, 51, 42, 287, 48, 59, 35, 54, 24, 183, 79, 44, 45, 65, 47, 89, 64, 76, 38, 94, 124, 32, 133, 87, 81, 128, 128, 59, 41, 103, 37, 88, 105, 90, 58, 44, 64, 229, 189, 164, 96, 119, 65, 61, 103, 204, 98, 94, 146, 86, 82, 66, 74, 90, 126, 96, 78, 129, 198, 186, 205, 168, 172, 65, 158, 128, 153, 95, 181, 93, 113, 109, 76, 164, 112, 99, 168, 94, 101, 141, 106, 130, 76, 81, 76, 168, 80, 168, 142, 64, 123, 104, 88, 104]\n",
      "[47, 79, 109, 20, 86, 27, 21, 11, 13, 123, 46, 11, 6, 33, 16, 14, 11, 42, 86, 134, 39, 16, 13, 19, 92, 7, 34, 133, 7, 13, 20, 4, 11, 28, 26, 13, 42, 13, 36, 18, 101, 92, 18, 8, 72, 27, 34, 15, 40, 8, 110, 25, 27, 13, 26, 123, 17, 115, 57, 8, 19, 120, 30, 121, 26, 61, 9, 11, 48, 52, 66, 61, 16, 5, 19, 6, 3, 34, 85, 65, 21, 206, 100, 269, 56, 228, 146, 11, 30, 23, 92, 99, 77, 69, 46, 58, 53, 29, 35, 9, 8, 68, 105, 61, 29, 13, 24, 21, 14, 64, 10, 50, 32, 29, 14, 7, 36, 35]\n",
      "[4, 4, 2, 1, 1, 5, 3, 3, 9, 5, 0, 3, 1, 6, 3, 1, 1, 4, 3, 3, 8, 2, 1, 1, 2, 3, 3, 8, 7, 1, 1, 0, 1, 9, 1, 3, 4, 2, 1, 3, 2, 3, 3, 7, 3, 5, 5, 2, 1, 5, 7, 3, 3, 1, 5, 4, 4, 0, 3, 4, 0, 3, 9, 2, 3, 6, 2, 0, 6, 11, 3, 3, 6, 1, 4, 1, 2, 6, 3, 4, 4, 2, 9, 7, 4, 4, 1, 4, 2, 1, 5, 3, 5, 5, 5, 3, 2, 4, 7, 1, 9, 4, 8, 11, 6, 8, 2, 2, 4, 10, 3, 9, 11, 2, 5, 2, 5, 3]\n"
     ]
    }
   ],
   "source": [
    "print(nNounsTotal)\n",
    "print(nAdjTotal)\n",
    "print(nVerbTotal)\n",
    "print(nNesTotal)\n",
    "print(xTotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "nPalavras = []\n",
    "nParagrafos = []\n",
    "\n",
    "for num in range (len(textos)):\n",
    "    nPalavras.append(len(textos[num]))\n",
    "    nParagrafos.append(len(textos2[num]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[345, 532, 913, 322, 557, 400, 400, 345, 577, 1129, 439, 311, 300, 516, 345, 449, 341, 799, 609, 714, 612, 375, 236, 438, 766, 381, 371, 1757, 330, 339, 292, 355, 203, 1052, 619, 363, 435, 487, 426, 678, 584, 529, 271, 691, 1079, 335, 1044, 583, 511, 668, 1031, 631, 348, 678, 342, 855, 768, 913, 515, 327, 389, 1702, 1153, 1124, 758, 947, 392, 443, 706, 1324, 810, 851, 816, 472, 697, 367, 406, 613, 973, 737, 576, 1228, 1441, 1452, 1252, 1249, 1309, 533, 1171, 651, 1341, 731, 1200, 687, 782, 749, 580, 1152, 749, 552, 1083, 685, 756, 1236, 779, 934, 554, 606, 564, 1219, 437, 1011, 987, 563, 788, 652, 608, 759]\n",
      "[9, 18, 33, 6, 13, 9, 7, 7, 9, 20, 11, 7, 7, 8, 8, 10, 7, 23, 9, 16, 9, 11, 7, 7, 10, 8, 7, 46, 6, 7, 7, 7, 8, 23, 8, 6, 8, 8, 7, 10, 14, 11, 7, 11, 13, 6, 14, 11, 8, 10, 35, 11, 11, 11, 8, 11, 19, 16, 15, 11, 12, 24, 17, 15, 13, 18, 6, 11, 10, 28, 11, 10, 10, 7, 10, 7, 7, 14, 17, 15, 9, 20, 24, 25, 14, 20, 20, 10, 19, 22, 15, 18, 18, 15, 17, 8, 8, 12, 18, 14, 17, 19, 12, 19, 9, 15, 9, 11, 12, 27, 10, 11, 17, 11, 28, 9, 10, 21]\n"
     ]
    }
   ],
   "source": [
    "print(nPalavras)\n",
    "print(nParagrafos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "titulos = []\n",
    "for i in range (len(textos2)):\n",
    "    titulos.append(textos2[i][0][:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "pergunta = []\n",
    "\n",
    "for titulo in titulos:\n",
    "    if(titulo.find('?') != -1):\n",
    "        pergunta.append(1)\n",
    "    else:\n",
    "        pergunta.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Livro - Professor, para que estudo isso?\n",
      "Coleção Contém Química\n",
      "O que é um podcast? É tipo rádio? Como faço para começar a ouvir?\n",
      "Nojo no mundo animal\n",
      "Prêmio Nobel 2017 – Microscopia Eletrônica com Criogenia\n",
      "Que seja eterno enquanto dure...\n",
      "O que dá cor aos fogos de artifício?\n",
      "Por que café descafeinado tem gosto e aroma de café?\n",
      "Arco-íris de sons? O que seria?\n",
      "[ENTREVISTA] Água em Marte: os próximos passos para a pesquisa espacial\n",
      "Lua de Sangue: entenda como funciona o Eclipse\n",
      "Por que o céu é azul?\n",
      "Estratégias de contracepção masculina\n",
      "Os alquimistas estão chegando\n",
      "E se acabássemos com o efeito estufa?\n",
      "A ciência que você vê, mas não percebe!\n",
      "Pílula anticoncepcional para o homem: é uma realidade?\n",
      "Se sentindo um impostor?\n",
      "O voo de Ícaro\n",
      "O Roubo da Medalha Fields\n",
      "Colesterol pode ser bom?\n",
      "Números imaginários… Mas o quê? Por quê?\n",
      "Viagra feminino existe?\n",
      "O CO2 é o grande vilão do efeito estufa?\n",
      "Crônica de uma tragédia anunciada: a morte assistida do Museu Nacional\n",
      "O que as vacas têm a ver com o efeito estufa?\n",
      "O sol faz bem para o seu coração\n",
      "Como ajudar a ciência?\n",
      "Pilhas de primeira e segunda geração\n",
      "A gente evolui, mas não progride!\n",
      "Todo remédio é uma droga?\n",
      "Posso beber água de chuva?\n",
      "Atenção colecionadores de conchas, últimas unidades!\n",
      "Preste atenção! Neurociência explica o que você viu mas não viu\n",
      "Resenha – O último teorema de Fermat\n",
      "Dopamina: a molécula da síndrome de Parkinson\n",
      "Prêmio Nobel em Medicina 2018 – Imunologia e Câncer\n",
      "Sábio era o Soneca\n",
      "Pesquisa, extensão e gerenciamento\n",
      "Microalgas?\n",
      "[Física e Química] Prêmio Nobel premia pesquisas com aplicações biológicas\n",
      "O Brasil nunca ganhou o Nobel, mas…\n",
      "O mistério da aurora\n",
      "O que é água alcalina?\n",
      "Quantos gramas?\n",
      "Sirius, a “estrela mais brilhante do céu noturno”, é inaugurado!\n",
      "O que (não) é quântica!\n",
      "Qual o papel do professor universitário?\n",
      "Natal, ano novo e o método científico\n",
      "Minha amiga samambaia! Tudo ouve, tudo vê!\n",
      "[RETROSPECTIVA] 10 fatos do mundo da ciência em 2018\n",
      "“Venho dos braços de Morfeu” – o que o ópio, a morfina e a heroína nos contam?\n",
      "Resenha – As dez maiores descobertas da medicina\n",
      "O POP não poupa ninguém\n",
      "Divulgação científica, jornalismo científico ou comunicação científica?\n",
      "ASTROEM\n",
      "Produtos de Reações Químicas\n",
      "Turma da Mônica e a ciência\n",
      "Resenha – O fim da eternidade – Isaac Asimov\n",
      "Lágrima\n",
      "Jogos deixam VOCÊ violento?\n",
      "Que delícia de mar, hein?!\n",
      "O que é um artigo científico?\n",
      "Uma breve história (da resistência) da ciência\n",
      "A posição do polo norte magnético da Terra foi redefinida\n",
      "Golden Show: Os museus do ouro na Colômbia\n",
      "Mudanças climáticas globais e a era dos walking deads\n",
      "Resenha – O menino que descobriu o vento\n",
      "A quantos graus ferve a água?\n",
      "Doação de Sangue: o que vem depois do fim da picada?\n",
      "Muito além de Mona Lisa, 500 anos de da Vinci\n",
      "Tempestade de 1 bilhão de volts é descoberta com telescópio de múons\n",
      "Maternidade na carreira acadêmica: depoimento\n",
      "Será que basta olhos para ver o arco-íris? Como os animais enxergam cores?\n",
      "“Tenho direito a quatro ligações!!!” – disse o Carbono\n",
      "A hora do sapo beber água, chega?\n",
      "O sapo não lava o pé, mas tem chulé?\n",
      "Aversão à perda e efeito dotação: uma pequena introdução\n",
      "1995: Stonewall – A luta pelo direito de amar\n",
      "Quando a ciência liberta: a pesquisa que separa preconceito de doença\n",
      "Microalgas para decoração\n",
      "A divulgação científica na UFABC\n",
      "Existe cura para o autismo?\n",
      "Se o homem foi à Lua em 1969, por que nunca voltou?\n",
      "Será que Einstein pode estar errado?\n",
      "2019 é o Ano Internacional da Tabela Periódica\n",
      "Museo de la Memoria y los Derechos Humanos\n",
      "Cores para além do colorido\n",
      "Investimentos públicos em ciência e tecnologia: alguns porquês sim\n",
      "“Mãe, porque a gente tem que tomar vacina?”\n",
      "Grafeno e o prêmio Nobel de Física de 2010\n",
      "Onde nenhum homem jamais esteve (ainda): Star Trek como precursor de invenções\n",
      "A importância da ciência básica e do investimento em ciência\n",
      "Ada Lovelace e os números de Bernoulli\n",
      "Quem foi Emmy Noether?\n",
      "Defensivos agrícolas ou agrotóxicos? Modernização ou vilania?\n",
      "Nobel de Medicina e Fisiologia de 2019\n",
      "Plástico: para sempre teu\n",
      "Além do POP, tem o HAP e o que essa sopa de letrinhas tem a ver com “o dia que virou noite”\n",
      "Por que as tatuagens são permanentes?\n",
      "Petróleo na praia: limpou tá limpo! Certo ou errado?\n",
      "Ostrava: cultura, história, ciência e tecnologia em uma só cidade\n",
      "O que falam sobre os jovens no Brasil não é sério\n",
      "Os Mitos e Mistérios de Saturno\n",
      "Por que usamos “cloro” (ou água sanitária) para limpar a nossa casa?\n",
      "O que acontece no seu corpo após tomar aquela cerveja gelada?\n",
      "Perigos de um cabelo liso e duradouro\n",
      "A química por trás da depressão\n",
      "Por que aquele saboroso cafezinho espanta o nosso sono?\n",
      "10 momentos da ciência em 2019\n",
      "O que são explicações exploráveis?\n",
      "A percepção dos brasileiros sobre a ciência\n",
      "A química dos sabores\n",
      "As especiarias e os aromas\n",
      "Existem infinitos maiores que outros?\n",
      "Seres Bioluminescentes\n",
      "O que tem a ver um tear com a era dos computadores?\n",
      "O Paradoxo de Simpson te mostra que nem tudo é o que parece\n"
     ]
    }
   ],
   "source": [
    "for i in range (len(textos2)):\n",
    "    print(textos2[i][0][:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto 117 - O Paradoxo de Simpson te mostra que nem tudo é o que parece\n",
      "Autor: Marcelo Pena\n",
      "Número de palavras: 759\n",
      "Número de parágrafos: 21\n",
      "Número de substantivos: 202\n",
      "Número de adjetivos: 46\n",
      "Número de verbos: 104\n",
      "Número de Entidades Nomeadas: 35\n",
      "É uma pergunta?: 0\n"
     ]
    }
   ],
   "source": [
    "i = 117\n",
    "print(f\"Texto {i} - {textos2[i][0][:-1]}\")\n",
    "print(f\"Autor: {textos2[i][-1][:-1]}\")\n",
    "print(f\"Número de palavras: {nPalavras[i]}\")\n",
    "print(f\"Número de parágrafos: {nParagrafos[i]}\") ####### Título e Autor contam como parágrafos\n",
    "print(f\"Número de substantivos: {nNounsTotal[i]}\")\n",
    "print(f\"Número de adjetivos: {nAdjTotal[i]}\")\n",
    "print(f\"Número de verbos: {nVerbTotal[i]}\")\n",
    "print(f\"Número de Entidades Nomeadas: {nNesTotal[i]}\")\n",
    "print(f\"É uma pergunta?: {pergunta[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "nomes = ['numPal', 'numPar','numSub', 'numAdj', 'numVrb', 'nemNEs','pergunta']\n",
    "df = pd.DataFrame(list(zip(nPalavras, nParagrafos, nNounsTotal, nAdjTotal, nVerbTotal, nNesTotal, pergunta)),columns=nomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     numPal  numPar  numSub  numAdj  numVrb  nemNEs  pergunta\n",
      "0       345       9     137      30      41      47         1\n",
      "1       532      18     223      37      60      79         0\n",
      "2       913      33     322      42     152     109         1\n",
      "3       322       6     120      20      43      20         0\n",
      "4       557      13     227      40      55      86         0\n",
      "5       400       9     116      25      53      27         0\n",
      "6       400       7     142      26      56      21         1\n",
      "7       345       7     103      14      54      11         1\n",
      "8       577       9     185      25      86      13         1\n",
      "9      1129      20     382      70     140     123         0\n",
      "10      439      11     140      26      60      46         0\n",
      "11      311       7      85      22      48      11         1\n",
      "12      300       7     103      43      31       6         0\n",
      "13      516       8     187      26      63      33         0\n",
      "14      345       8     100      25      50      16         1\n",
      "15      449      10     139      24      64      14         0\n",
      "16      341       7     102      38      40      11         1\n",
      "17      799      23     198      49     145      42         1\n",
      "18      609       9     219      40      75      86         0\n",
      "19      714      16     264      38      78     134         0\n",
      "20      612       9     224      63      68      39         1\n",
      "21      375      11     118      35      73      16         1\n",
      "22      236       7      84      27      32      13         1\n",
      "23      438       7     134      38      64      19         1\n",
      "24      766      10     251      38     105      92         0\n",
      "25      381       8     114      22      51       7         1\n",
      "26      371       7     141      40      42      34         0\n",
      "27     1757      46     530     104     287     133         1\n",
      "28      330       6     102      32      48       7         0\n",
      "29      339       7      99      26      59      13         0\n",
      "..      ...     ...     ...     ...     ...     ...       ...\n",
      "88     1171      19     329     125     158      30         0\n",
      "89      651      22     172      32     128      23         1\n",
      "90     1341      15     431     104     153      92         0\n",
      "91      731      18     262      33      95      99         0\n",
      "92     1200      18     425      65     181      77         0\n",
      "93      687      15     232      46      93      69         0\n",
      "94      782      17     225      60     113      46         1\n",
      "95      749       8     233      65     109      58         1\n",
      "96      580       8     210      46      76      53         0\n",
      "97     1152      12     280      85     164      29         0\n",
      "98      749      18     205      70     112      35         0\n",
      "99      552      14     155      34      99       9         1\n",
      "100    1083      17     320      64     168       8         1\n",
      "101     685      19     226      65      94      68         0\n",
      "102     756      12     255      61     101     105         0\n",
      "103    1236      19     358     108     141      61         0\n",
      "104     779       9     230      69     106      29         1\n",
      "105     934      15     285      71     130      13         1\n",
      "106     554       9     150      52      76      24         0\n",
      "107     606      11     202      45      81      21         0\n",
      "108     564      12     173      34      76      14         1\n",
      "109    1219      27     383      85     168      64         0\n",
      "110     437      10     107      36      80      10         1\n",
      "111    1011      11     246      87     168      50         0\n",
      "112     987      17     285      82     142      32         0\n",
      "113     563      11     182      67      64      29         0\n",
      "114     788      28     187      72     123      14         1\n",
      "115     652       9     195      61     104       7         0\n",
      "116     608      10     182      35      88      36         1\n",
      "117     759      21     202      46     104      35         0\n",
      "\n",
      "[118 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)\n",
    "df.to_csv(r'C:\\Users\\Acer\\Dropbox\\UFABC\\PGC\\dataframe.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "admin\n",
      "admin\n",
      "Marcelo Pena\n",
      "Camilo Angelucci\n",
      "Daniele Araújo\n",
      "Camilo Angelucci\n",
      "Ivanise Gaubeur, Naomi Akiba\n",
      "Emily Takeuchi\n",
      "Cassiano Aono\n",
      "Renato Cunha\n",
      "Manuela Rodrigues\n",
      "Fábio Furlan\n",
      "Carlos Alberto-Silva\n",
      "Victoria Baptista Dias Miotto, Ronei Miotto\n",
      "Janaína Garcia\n",
      "Vanessa Verdade\n",
      "Carlos Alberto-Silva\n",
      "Marcelo Pena\n",
      "Marcelo Leigui\n",
      "Camilo Angelucci\n",
      "Hugo Suffredini\n",
      "Camila Vieira\n",
      "Amedea Seabra\n",
      "Janaína Garcia\n",
      "Fabiana Rodrigues Costa Nunes\n",
      "Janaína Garcia\n",
      "Amedea Seabra\n",
      "Marcelo Pena\n",
      "Hugo Suffredini\n",
      "Vanessa Verdade\n",
      "Daniele Araújo\n",
      "Naomi Akiba\n",
      "Vanessa Verdade\n",
      "Gloria Santucci\n",
      "Attalya Felix\n",
      "Hugo Suffredini\n",
      "Daniele Araújo\n",
      "Marcelo S. Caetano, Raquel Fornari\n",
      "Carlos Alberto-Silva, Vanessa Verdade\n",
      "Livia Seno Ferreira Camargo\n",
      "Renato Cunha\n",
      "Cassiano Aono\n",
      "Fábio Furlan\n",
      "Janaína Garcia\n",
      "Marcelo Leigui\n",
      "Fábio Furlan\n",
      "Gabriela Dias, Michele Salvador\n",
      "Hugo Suffredini, Paula Homem de Mello\n",
      "Cassiano Aono\n",
      "Vanessa Verdade\n",
      "Renato Cunha\n",
      "Daniele Araújo\n",
      "Daniele Araújo\n",
      "Felipe Cesar\n",
      "Renato Cunha\n",
      "Wesley Góis\n",
      "Jhonathan Souza\n",
      "Marcelo Pena\n",
      "Jhonathan Souza\n",
      "Marcelo Pena\n",
      "Marcelo Pena\n",
      "Felipe Cesar, Gabriela Dias\n",
      "Paula Homem de Mello, Hugo Suffredini, Felipe Cesar\n",
      "Michele Salvador, Gabriela Dias\n",
      "Michele Salvador\n",
      "Paula Homem de Mello\n",
      "Vanessa Verdade\n",
      "Gabriela Dias\n",
      "Rodrigo Papai\n",
      "Cassiano Aono, Felipe Cesar\n",
      "Pedro Henrique Thiayamiti Santos, João Henrique Quintino Palhares\n",
      "Marcelo Leigui\n",
      "Vanessa Verdade\n",
      "Vanessa Verdade\n",
      "Jhonathan Souza\n",
      "Vanessa Verdade\n",
      "Vanessa Verdade\n",
      "Davi Duarte, Everson Silva\n",
      "Felipe Cesar\n",
      "Felipe Cesar\n",
      "Livia Seno Ferreira Camargo\n",
      "Marcelo Pena\n",
      "Vanessa Ludovico, Felipe Cesar\n",
      "Marcelo Pena\n",
      "Alysson Fabio Ferrari\n",
      "Patrícia Dantoni, Paula Homem de Mello\n",
      "Felipe Cesar, Paula Homem de Mello\n",
      "Paula Homem de Mello\n",
      "Felipe Cesar, Cassiano Aono\n",
      "Vanessa Verdade\n",
      "Luis Henrique de Lima\n",
      "Renato Cunha\n",
      "Marcelo Pena\n",
      "Mónica López\n",
      "Cassiano Aono, Laiz G. Chagas, Thariny Oliveira\n",
      "Vanessa Verdade\n",
      "Gabriela Dias, Cleiton Maciel\n",
      "Gabriela Dias\n",
      "Michele Salvador, Gabriela Dias\n",
      "Gabriel Mol\n",
      "Janaína Garcia\n",
      "Renato Cunha\n",
      "Cassiano Aono\n",
      "Jhonathan Souza\n",
      "Lanna Emilli Barbosa Lucchetti, Diego de Oliveira Rogério, André Luís Pesquero de Melo, Robert Maiory Alarcon Flores\n",
      "Jéssica Oliveira Aguiar Perez, Marcos Ferrer Lima, Renato Cunha\n",
      "Leonardo Henrique de Macedo, Vitória Aparecida Procópio, Miguel Tabanez, Leonardo Martins Carneiro\n",
      "Alícia Fortunato Batista, Susan Meirelles Dantas, Gabrielle Mathias Reis, Wagner José Odilon dos Santos\n",
      "Matheus Lopes Silva, Helen Reis Modesto, Wellington Diego da Ascenção\n",
      "Marcelo Pena\n",
      "Marcelo Pena\n",
      "Marcelo Pena\n",
      "Paula Homem de Mello, Hugo Suffredini\n",
      "Paula Homem de Mello, Hugo Suffredini\n",
      "Marcelo Pena\n",
      "Livia Seno Ferreira Camargo\n",
      "Mónica López\n",
      "Marcelo Pena\n"
     ]
    }
   ],
   "source": [
    "for i in range (len(textos2)):\n",
    "    print(textos2[i][-1][:-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
